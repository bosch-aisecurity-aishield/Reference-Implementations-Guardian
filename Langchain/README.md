# 🔐 AI Security in LangChain Applications

## 📜 Overview

With the rapid expansion of AI applications, securing AI interactions is essential to prevent misuse and enforce content policies. This project demonstrates a **reference implementation for AI security using LangChain** and the **Guardian API** for policy-compliant workflows. By embedding compliance checks into each step, this setup ensures robust, controlled AI interactions.

## 🦜️🔗 Why LangChain?

**LangChain** is a flexible framework that enables developers to build powerful chains of language models integrated with tools, memory, and APIs. Its modular structure allows seamless customization, making it ideal for enforcing compliance in complex, real-world applications. Learn more about **Langchain** ([here](https://www.langchain.com))

## 🛡️ Why is Security Paramount?

Unchecked AI models can produce harmful or non-compliant content, posing security risks. Integrating a security layer through LangChain ensures safe usage by validating input and output, securing user interactions.

📜 To know more about 🤖  **Agentic AI system** and 🛡️**AI Agentic Security** read this [primer](../LearnAgenticAISecurity/agentic_ai_security.md).

## 💻  Reference Implementation

For a secure AI setup, you can explore integrations like **Ollama**  with **Guardian API**  within **Langchain** framework to monitor compliance and guardrail interactions.

- **Ollama** : [GuardedOllamaChain](Ollama_Simplechain_guardian) implementation with example of chatbot chain.

## 📅 **[Schedule a Call Here!](https://share-eu1.hsforms.com/1er3vym0FRA-r_B2ZnG5OWQffb9n?__hstc=138249519.4d817d58bf2f28287881f1a4495c2daa.1682320777326.1688113936277.1688634393681.37&__hssc=138249519.1.1688634393681&__hsfp=524412920)**

## ⚖️ License

This project is licensed under MIT License.
